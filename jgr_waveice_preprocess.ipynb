{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Vincent T. Cooper  \n",
    "\n",
    "Date: 1 October 2021  \n",
    "\n",
    "Data preprocessing supporting the analysis in JGR Oceans manuscript submission.  \n",
    "\n",
    "Purpose 1: Take model output from new run of Roach et al. (2019) coupled wave-ice model that has hourly coupling, and reformat the model output so that it is consistent with the earlier version of the run which had daily coupling and is reported in the Roach et al. (2019) paper. This preprocessing enables use of analysis code developed prior to the new run.\n",
    "\n",
    "Purpose 2: Calculate the distance inside the ice edge for all model output.\n",
    "\n",
    "Purpose 3: Reduce model output only to the central Beaufort region of interest to eliminate extraneous burden on data loading. Saving this down and loading only the reduced dataset in the analysis notebook is more nimble.\n",
    "\n",
    "VTC note: original preprocessing done in waveice_coupled_2021.ipynb, this is a clean version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/u/home/vcooper/miniconda3/envs/camus/lib/python3.7/site-packages/xarray/conventions.py:500: SerializationWarning: variable 'sithick' has multiple fill values {1e+20, 1e+20}, decoding all values to NaN.\n",
      "  decode_timedelta=decode_timedelta,\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "# from matplotlib.colors import DivergingNorm\n",
    "# import matplotlib.patches as patches\n",
    "%matplotlib inline\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import seaborn as sns; sns.set(color_codes=False)\n",
    "import cmocean\n",
    "# from windrose import WindroseAxes\n",
    "import warnings\n",
    "\n",
    "import cftime\n",
    "import datetime\n",
    "\n",
    "\n",
    "plt.rcParams['xtick.bottom'] = True # keep my tick marks\n",
    "plt.rcParams['ytick.left'] = True\n",
    "plt.rcParams['font.size'] = 18\n",
    "plt.rcParams[\"font.family\"] = \"Arial\"\n",
    "# plt.rcParams['figure.figsize'] = 12,8\n",
    "# mpl.rcParams['figure.dpi'] = 300 # activate for presentation quality\n",
    "\n",
    "from sklearn.metrics.pairwise import haversine_distances\n",
    "\n",
    "## this is a dummy grid that has the right conventions\n",
    "grid = xr.open_dataset(\n",
    "    '/glade/work/vcooper/grid_ref/sithick_SImon_CESM2_piControl_r1i1p1f1_gn_110001-120012.nc')\n",
    "\n",
    "## circle boundary for plotting\n",
    "# theta = np.linspace(0, 2*np.pi, 100)\n",
    "# center, radius = [0.5, 0.5], 0.5\n",
    "# verts = np.vstack([np.sin(theta), np.cos(theta)]).T\n",
    "# circle = mpl.path.Path(verts * radius + center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define function to calculate distance inside ice edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Updated version to 15% ice concentration threshold\n",
    "def icedistance(iceconc_input):\n",
    "    # turn icefracs to numpy array\n",
    "    icefracsnp = iceconc_input.values\n",
    "#     lats = iceconc_input.TLAT.values # cice version\n",
    "#     lons = iceconc_input.TLON.values # cice version\n",
    "    lats = iceconc_input.latitude.values # wavewatch version\n",
    "    lons = iceconc_input.longitude.values # wavewatch version\n",
    "\n",
    "\n",
    "    # create array to hold the distances\n",
    "    distances = icefracsnp.copy() # same size array as the evaluated data\n",
    "    distances -= distances # make zeros or nan; we will keep these values for cells that don't need a calc\n",
    "\n",
    "    \n",
    "    ##### GET OPEN WATER -> WATER/ICE EDGE LOCATIONS #####\n",
    "    \n",
    "    # get all open water locations except at edge of domain to avoid computation breaking\n",
    "    icefracsnp_noborder = icefracsnp[1:-1,1:-1] # exclude borders for open water checking neighbors\n",
    "    locations_openw = np.transpose(np.where(icefracsnp_noborder<0.15))\n",
    "    locations_openw += 1 # adjust indices for the border exclusion\n",
    "\n",
    "    # create 4 arrays, each represents the offset of open water location in coords by 1 unit\n",
    "    latp1 = np.append(locations_openw[:,0]+1,locations_openw[:,1]).reshape(locations_openw.shape,order='F')\n",
    "    latm1 = np.append(locations_openw[:,0]-1,locations_openw[:,1]).reshape(locations_openw.shape,order='F')\n",
    "    lonp1 = np.append(locations_openw[:,0],locations_openw[:,1]+1).reshape(locations_openw.shape,order='F')\n",
    "    lonm1 = np.append(locations_openw[:,0],locations_openw[:,1]-1).reshape(locations_openw.shape,order='F')\n",
    "\n",
    "    # get max icefrac of 4 neighbor cells at each open water cell\n",
    "    iceneighbormax = np.nanmax(np.stack((icefracsnp[latp1[:,0],latp1[:,1]],\n",
    "                                         icefracsnp[lonm1[:,0],lonm1[:,1]],\n",
    "                                         icefracsnp[lonp1[:,0],lonp1[:,1]],\n",
    "                                         icefracsnp[latm1[:,0],latm1[:,1]])),axis=0)\n",
    "\n",
    "    # get index of the open water cells with ice neighbor>15% # these are values for which we will calc distance\n",
    "    wateredge = locations_openw[np.where(iceneighbormax>0.15)]\n",
    "    wateredgeT = wateredge.T\n",
    "    wateredgelatlon = np.array([[lats[wateredgeT[0],wateredgeT[1]]],\n",
    "                                [lons[wateredgeT[0],wateredgeT[1]]]]).squeeze().T # Nx2 matrix of lat,lon\n",
    "    \n",
    "    ##### CALCULATION OF DISTANCES #####\n",
    "    \n",
    "    # get all cell locations with ice > 15%\n",
    "    icewhere = np.where(icefracsnp>0.15)\n",
    "    icecells = np.transpose(icewhere) # index by array position\n",
    "    icelatlon = np.array([[lats[icewhere]],\n",
    "                          [lons[icewhere]]]).squeeze().T # Nx2 matrix of lat,lon\n",
    "\n",
    "    # calculate minimum distance\n",
    "    mindist = haversine_distances(np.deg2rad(icelatlon),\n",
    "                                  np.deg2rad(wateredgelatlon)).min(axis=1)*6371000/1000 # x by Radius-earth for km\n",
    "    \n",
    "    icecellsT = np.transpose(icecells) # transpose for vectorized indexing\n",
    "    distances[icecellsT[0],icecellsT[1]] = mindist # put mindist into each grid point\n",
    "\n",
    "\n",
    "    return(distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load new hourly coupled model run and calculate $\\Delta ^{dist}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I loaded each year from 2012-2019 as a separate dataset, then looped through each year for preprocessing and saved down each year of distance data separately. Concatenated them later on.  \n",
    "\n",
    "First, I'll show how this is done for just the 2012-01-01 data for example, then I'll include the actual loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_path = '/glade/scratch/bitz/cesm23iws1tsks/run/hourly/'\n",
    "path_pre_year = hourly_path + 'cesm23iws1tsks.ww3.hi.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load data, add latitude and longitude coords\n",
    "year_sel = '2012'\n",
    "ww_temp = xr.open_mfdataset(path_pre_year + year_sel + '-01-01*.nc',\n",
    "                           combine='nested',concat_dim='time')\n",
    "ww_temp['latitude'] = (['NY','NX'],grid.lat.values)\n",
    "ww_temp['longitude'] = (['NY','NX'],grid.lon.values)\n",
    "ww_temp = ww_temp.set_coords(['latitude','longitude'])\n",
    "\n",
    "## initialize distances array\n",
    "ice_conc_data = ww_temp.ICE\n",
    "distances = np.zeros(ice_conc_data.shape)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\") ## ignore warnings from div by zero leading to nans\n",
    "\n",
    "    ## loop through time, calculating distance from ice edge\n",
    "    for i in range(len(distances)):\n",
    "        distances[i] = icedistance(ice_conc_data[i])\n",
    "\n",
    "## convert to xarray\n",
    "distances_da = xr.DataArray(distances, dims=ice_conc_data.dims,coords=ice_conc_data.coords)\n",
    "\n",
    "## save netcdf (commented out bc we don't actually use this, it is just a single day example)\n",
    "# new_filename = '/glade/scratch/vcooper/waveice_analysis/distance_calc/cesm23iws1tsks.ww3.hi.icedistance.' + year + '.nc'\n",
    "# print ('saving to ', new_filename)\n",
    "\n",
    "# distances_da.to_netcdf(path=new_filename)\n",
    "# print ('finished saving')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set up a function to compute distances and saves files all at once\n",
    "def distance_calc_output(year_sel):\n",
    "    ## set up file loading\n",
    "    hourly_path = '/glade/scratch/bitz/cesm23iws1tsks/run/hourly/'\n",
    "    path_pre_year = hourly_path + 'cesm23iws1tsks.ww3.hi.'\n",
    "\n",
    "    ## load data, add latitude and longitude coords\n",
    "    #year_sel = '2012'\n",
    "    ww_temp = xr.open_mfdataset(path_pre_year + year_sel + '*.nc',\n",
    "                               combine='nested',concat_dim='time')\n",
    "    ww_temp['latitude'] = (['NY','NX'],grid.lat.values)\n",
    "    ww_temp['longitude'] = (['NY','NX'],grid.lon.values)\n",
    "    ww_temp = ww_temp.set_coords(['latitude','longitude'])\n",
    "\n",
    "    ## initialize distances\n",
    "    ice_conc_data = ww_temp.ICE\n",
    "    distances = np.zeros(ice_conc_data.shape)\n",
    "\n",
    "    ## loop through time\n",
    "    for i in range(len(distances)):\n",
    "        print(str(i+1) + ' / ' + str(len(distances)))\n",
    "        distances[i] = icedistance(ice_conc_data[i])\n",
    "\n",
    "    ## convert to xarray\n",
    "    distances_da = xr.DataArray(distances, dims=ice_conc_data.dims,coords=ice_conc_data.coords)\n",
    "\n",
    "    ## save netcdf\n",
    "    new_filename = '/glade/scratch/vcooper/waveice_analysis/distance_calc/cesm23iws1tsks.ww3.hi.icedistance.' + year_sel + '.nc'\n",
    "    print ('saving to ', new_filename)\n",
    "\n",
    "    distances_da.to_netcdf(path=new_filename)\n",
    "    print ('finished saving')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dont run this, it is here for reference but will take v long time\n",
    "yrs = np.array([np.arange(2012,2019+1)])\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "    for yr in yrs:\n",
    "        print(str(yr))\n",
    "        distance_calc_output(str(yr))\n",
    "        \n",
    "# with this cell done, we have hourly distance inside the ice edge global data \n",
    "# saved separately for each year 2012-2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now load the hourly model data and save versions grouped by year\n",
    "Still with hourly resolution. This step is just intermediate so that I don't have to waste time loading hourly data and concatting it every time I want to do analysis.\n",
    "\n",
    "### We are working towards creating a single central Beaufort file with all wavewatch and $\\Delta ^{dist}$ data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012-01-01T00:00:00.000000000 2012-12-31T23:00:00.000000000\n",
      "8760\n",
      "2013-01-01T00:00:00.000000000 2013-12-31T23:00:00.000000000\n",
      "8760\n",
      "2014-01-01T00:00:00.000000000 2014-12-31T23:00:00.000000000\n",
      "8760\n",
      "2015-01-01T00:00:00.000000000 2015-12-31T23:00:00.000000000\n",
      "8760\n",
      "2016-01-01T00:00:00.000000000 2016-12-31T23:00:00.000000000\n",
      "8760\n",
      "2017-01-01T00:00:00.000000000 2017-12-31T23:00:00.000000000\n",
      "8760\n",
      "2018-01-01T00:00:00.000000000 2018-12-31T23:00:00.000000000\n",
      "8760\n",
      "2019-01-01T00:00:00.000000000 2019-12-31T23:00:00.000000000\n",
      "8760\n"
     ]
    }
   ],
   "source": [
    "#### First, need to create time variable bc wavewatch doesn't have actual time data.\n",
    "timetemp = np.datetime64('2012-01-01T00:00:00.000000000') + np.arange(8760+24)*np.timedelta64(1,'h')\n",
    "print(timetemp[0],timetemp[-1])\n",
    "timetemp_pd = pd.to_datetime(timetemp)\n",
    "## drop leap years\n",
    "timetemp_pd = timetemp_pd[~((timetemp_pd.month == 2) & (timetemp_pd.day == 29))]\n",
    "time2012 = np.array([i.to_datetime64() for i in timetemp_pd])\n",
    "print(time2012.size)\n",
    "\n",
    "timetemp = np.datetime64('2013-01-01T00:00:00.000000000') + np.arange(8760)*np.timedelta64(1,'h')\n",
    "print(timetemp[0],timetemp[-1])\n",
    "timetemp_pd = pd.to_datetime(timetemp)\n",
    "## drop leap years\n",
    "timetemp_pd = timetemp_pd[~((timetemp_pd.month == 2) & (timetemp_pd.day == 29))]\n",
    "time2013 = np.array([i.to_datetime64() for i in timetemp_pd])\n",
    "print(time2013.size)\n",
    "\n",
    "timetemp = np.datetime64('2014-01-01T00:00:00.000000000') + np.arange(8760)*np.timedelta64(1,'h')\n",
    "print(timetemp[0],timetemp[-1])\n",
    "timetemp_pd = pd.to_datetime(timetemp)\n",
    "## drop leap years\n",
    "timetemp_pd = timetemp_pd[~((timetemp_pd.month == 2) & (timetemp_pd.day == 29))]\n",
    "time2014 = np.array([i.to_datetime64() for i in timetemp_pd])\n",
    "print(time2014.size)\n",
    "\n",
    "timetemp = np.datetime64('2015-01-01T00:00:00.000000000') + np.arange(8760)*np.timedelta64(1,'h')\n",
    "print(timetemp[0],timetemp[-1])\n",
    "timetemp_pd = pd.to_datetime(timetemp)\n",
    "## drop leap years\n",
    "timetemp_pd = timetemp_pd[~((timetemp_pd.month == 2) & (timetemp_pd.day == 29))]\n",
    "time2015 = np.array([i.to_datetime64() for i in timetemp_pd])\n",
    "print(time2015.size)\n",
    "\n",
    "timetemp = np.datetime64('2016-01-01T00:00:00.000000000') + np.arange(8760+24)*np.timedelta64(1,'h')\n",
    "print(timetemp[0],timetemp[-1])\n",
    "timetemp_pd = pd.to_datetime(timetemp)\n",
    "## drop leap years\n",
    "timetemp_pd = timetemp_pd[~((timetemp_pd.month == 2) & (timetemp_pd.day == 29))]\n",
    "time2016 = np.array([i.to_datetime64() for i in timetemp_pd])\n",
    "print(time2016.size)\n",
    "\n",
    "timetemp = np.datetime64('2017-01-01T00:00:00.000000000') + np.arange(8760)*np.timedelta64(1,'h')\n",
    "print(timetemp[0],timetemp[-1])\n",
    "timetemp_pd = pd.to_datetime(timetemp)\n",
    "## drop leap years\n",
    "timetemp_pd = timetemp_pd[~((timetemp_pd.month == 2) & (timetemp_pd.day == 29))]\n",
    "time2017 = np.array([i.to_datetime64() for i in timetemp_pd])\n",
    "print(time2017.size)\n",
    "\n",
    "timetemp = np.datetime64('2018-01-01T00:00:00.000000000') + np.arange(8760)*np.timedelta64(1,'h')\n",
    "print(timetemp[0],timetemp[-1])\n",
    "timetemp_pd = pd.to_datetime(timetemp)\n",
    "## drop leap years\n",
    "timetemp_pd = timetemp_pd[~((timetemp_pd.month == 2) & (timetemp_pd.day == 29))]\n",
    "time2018 = np.array([i.to_datetime64() for i in timetemp_pd])\n",
    "print(time2018.size)\n",
    "\n",
    "timetemp = np.datetime64('2019-01-01T00:00:00.000000000') + np.arange(8760)*np.timedelta64(1,'h')\n",
    "print(timetemp[0],timetemp[-1])\n",
    "timetemp_pd = pd.to_datetime(timetemp)\n",
    "## drop leap years\n",
    "timetemp_pd = timetemp_pd[~((timetemp_pd.month == 2) & (timetemp_pd.day == 29))]\n",
    "time2019 = np.array([i.to_datetime64() for i in timetemp_pd])\n",
    "print(time2019.size)\n",
    "\n",
    "alltimes_2012_2019 = np.hstack([time2012,time2013,time2014,time2015,\n",
    "                                time2016,time2017,time2018,time2019])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## each section takes almost 10 minutes to concatenate\n",
    "hourly_path = '/glade/scratch/bitz/cesm23iws1tsks/run/hourly/'\n",
    "path_pre_year = hourly_path + 'cesm23iws1tsks.ww3.hi.'\n",
    "temppath = path_pre_year + '2012'\n",
    "ww2012 = xr.open_mfdataset(temppath + '*.nc',combine='nested',concat_dim='time')\n",
    "print('done with 12')\n",
    "\n",
    "hourly_path = '/glade/scratch/bitz/cesm23iws1tsks/run/hourly/'\n",
    "path_pre_year = hourly_path + 'cesm23iws1tsks.ww3.hi.'\n",
    "temppath = path_pre_year + '2013'\n",
    "ww2013 = xr.open_mfdataset(temppath + '*.nc',combine='nested',concat_dim='time')\n",
    "\n",
    "hourly_path = '/glade/scratch/bitz/cesm23iws1tsks/run/hourly/'\n",
    "path_pre_year = hourly_path + 'cesm23iws1tsks.ww3.hi.'\n",
    "temppath = path_pre_year + '2014'\n",
    "ww2014 = xr.open_mfdataset(temppath + '*.nc',combine='nested',concat_dim='time')\n",
    "\n",
    "hourly_path = '/glade/scratch/bitz/cesm23iws1tsks/run/hourly/'\n",
    "path_pre_year = hourly_path + 'cesm23iws1tsks.ww3.hi.'\n",
    "temppath = path_pre_year + '2015'\n",
    "ww2015 = xr.open_mfdataset(temppath + '*.nc',combine='nested',concat_dim='time')\n",
    "\n",
    "hourly_path = '/glade/scratch/bitz/cesm23iws1tsks/run/hourly/'\n",
    "path_pre_year = hourly_path + 'cesm23iws1tsks.ww3.hi.'\n",
    "temppath = path_pre_year + '2016'\n",
    "ww2016 = xr.open_mfdataset(temppath + '*.nc',combine='nested',concat_dim='time')\n",
    "print('done with 15 and 16')\n",
    "\n",
    "hourly_path = '/glade/scratch/bitz/cesm23iws1tsks/run/hourly/'\n",
    "path_pre_year = hourly_path + 'cesm23iws1tsks.ww3.hi.'\n",
    "temppath = path_pre_year + '2017'\n",
    "ww2017 = xr.open_mfdataset(temppath + '*.nc',combine='nested',concat_dim='time')\n",
    "\n",
    "hourly_path = '/glade/scratch/bitz/cesm23iws1tsks/run/hourly/'\n",
    "path_pre_year = hourly_path + 'cesm23iws1tsks.ww3.hi.'\n",
    "temppath = path_pre_year + '2018'\n",
    "ww2018 = xr.open_mfdataset(temppath + '*.nc',combine='nested',concat_dim='time')\n",
    "\n",
    "hourly_path = '/glade/scratch/bitz/cesm23iws1tsks/run/hourly/'\n",
    "path_pre_year = hourly_path + 'cesm23iws1tsks.ww3.hi.'\n",
    "temppath = path_pre_year + '2019'\n",
    "ww2019 = xr.open_mfdataset(temppath + '*.nc',combine='nested',concat_dim='time')\n",
    "print('done with 17-19')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## now add time variable to each dataset\n",
    "ww2012['time'] = time2012\n",
    "ww2013['time'] = time2013\n",
    "ww2014['time'] = time2014\n",
    "ww2015['time'] = time2015\n",
    "ww2016['time'] = time2016\n",
    "ww2017['time'] = time2017\n",
    "ww2018['time'] = time2018\n",
    "ww2019['time'] = time2019\n",
    "\n",
    "## load the distance data we calculated earlier\n",
    "tempds12 = xr.open_dataarray('/glade/scratch/vcooper/waveice_analysis/distance_calc/cesm23iws1tsks.ww3.hi.icedistance.2012.nc')\n",
    "tempds13 = xr.open_dataarray('/glade/scratch/vcooper/waveice_analysis/distance_calc/cesm23iws1tsks.ww3.hi.icedistance.2013.nc')\n",
    "tempds14 = xr.open_dataarray('/glade/scratch/vcooper/waveice_analysis/distance_calc/cesm23iws1tsks.ww3.hi.icedistance.2014.nc')\n",
    "tempds15 = xr.open_dataarray('/glade/scratch/vcooper/waveice_analysis/distance_calc/cesm23iws1tsks.ww3.hi.icedistance.2015.nc')\n",
    "tempds16 = xr.open_dataarray('/glade/scratch/vcooper/waveice_analysis/distance_calc/cesm23iws1tsks.ww3.hi.icedistance.2016.nc')\n",
    "tempds17 = xr.open_dataarray('/glade/scratch/vcooper/waveice_analysis/distance_calc/cesm23iws1tsks.ww3.hi.icedistance.2017.nc')\n",
    "tempds18 = xr.open_dataarray('/glade/scratch/vcooper/waveice_analysis/distance_calc/cesm23iws1tsks.ww3.hi.icedistance.2018.nc')\n",
    "tempds19 = xr.open_dataarray('/glade/scratch/vcooper/waveice_analysis/distance_calc/cesm23iws1tsks.ww3.hi.icedistance.2019.nc')\n",
    "\n",
    "tempds12['time'] = time2012\n",
    "tempds13['time'] = time2013\n",
    "tempds14['time'] = time2014\n",
    "tempds15['time'] = time2015\n",
    "tempds16['time'] = time2016\n",
    "tempds17['time'] = time2017\n",
    "tempds18['time'] = time2018\n",
    "tempds19['time'] = time2019\n",
    "\n",
    "## i accidentally named this distance in original version, fixed here if i need to redo\n",
    "## add the distance from ice edge variable to the wavewatch data\n",
    "ww2012['dist'] = tempds12\n",
    "ww2013['dist'] = tempds13\n",
    "ww2014['dist'] = tempds14\n",
    "ww2015['dist'] = tempds15\n",
    "ww2016['dist'] = tempds16\n",
    "ww2017['dist'] = tempds17\n",
    "ww2018['dist'] = tempds18\n",
    "ww2019['dist'] = tempds19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save down all of this hourly data grouped by year so i can load it faster later on\n",
    "%%time\n",
    "## save preprocessed dataset separately, then concat them later\n",
    "## each of these saves takes 2h to 2h30m\n",
    "new_filename = '/glade/scratch/vcooper/waveice_analysis/cesm23iws1tsks.ww3.hi.2012.nc'\n",
    "print('saving to ', new_filename)\n",
    "\n",
    "ww2012.to_netcdf(path=new_filename)\n",
    "ww2012.close()\n",
    "tempds12.close()\n",
    "print('finished saving')\n",
    "\n",
    "new_filename = '/glade/scratch/vcooper/waveice_analysis/cesm23iws1tsks.ww3.hi.2013.nc'\n",
    "print('saving to ', new_filename)\n",
    "\n",
    "ww2013.to_netcdf(path=new_filename)\n",
    "ww2013.close()\n",
    "tempds13.close()\n",
    "print('finished saving')\n",
    "\n",
    "\n",
    "##\n",
    "new_filename = '/glade/scratch/vcooper/waveice_analysis/cesm23iws1tsks.ww3.hi.2014.nc'\n",
    "print('saving to ', new_filename)\n",
    "\n",
    "ww2014.to_netcdf(path=new_filename)\n",
    "ww2014.close()\n",
    "tempds14.close()\n",
    "print('finished saving')\n",
    "\n",
    "##\n",
    "new_filename = '/glade/scratch/vcooper/waveice_analysis/cesm23iws1tsks.ww3.hi.2015.nc'\n",
    "print('saving to ', new_filename)\n",
    "\n",
    "ww2015.to_netcdf(path=new_filename)\n",
    "ww2015.close()\n",
    "tempds15.close()\n",
    "print('finished saving')\n",
    "\n",
    "##\n",
    "new_filename = '/glade/scratch/vcooper/waveice_analysis/cesm23iws1tsks.ww3.hi.2016.nc'\n",
    "print('saving to ', new_filename)\n",
    "\n",
    "ww2016.to_netcdf(path=new_filename)\n",
    "ww2016.close()\n",
    "tempds16.close()\n",
    "print('finished saving')\n",
    "\n",
    "##\n",
    "new_filename = '/glade/scratch/vcooper/waveice_analysis/cesm23iws1tsks.ww3.hi.2017.nc'\n",
    "print('saving to ', new_filename)\n",
    "\n",
    "ww2017.to_netcdf(path=new_filename)\n",
    "ww2017.close()\n",
    "tempds17.close()\n",
    "print('finished saving')\n",
    "\n",
    "##\n",
    "new_filename = '/glade/scratch/vcooper/waveice_analysis/cesm23iws1tsks.ww3.hi.2018.nc'\n",
    "print('saving to ', new_filename)\n",
    "\n",
    "ww2018.to_netcdf(path=new_filename)\n",
    "ww2018.close()\n",
    "tempds18.close()\n",
    "print('finished saving')\n",
    "\n",
    "##\n",
    "new_filename = '/glade/scratch/vcooper/waveice_analysis/cesm23iws1tsks.ww3.hi.2019.nc'\n",
    "print('saving to ', new_filename)\n",
    "\n",
    "ww2019.to_netcdf(path=new_filename)\n",
    "ww2019.close()\n",
    "tempds19.close()\n",
    "print('finished saving')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## now load in the data for each year and concatenate a reduced version of the dataset\n",
    "## that only includes the central beaufort region\n",
    "ww2012 = xr.open_dataset('/glade/scratch/vcooper/waveice_analysis/cesm23iws1tsks.ww3.hi.2012.nc')\n",
    "ww2013 = xr.open_dataset('/glade/scratch/vcooper/waveice_analysis/cesm23iws1tsks.ww3.hi.2013.nc')\n",
    "ww2014 = xr.open_dataset('/glade/scratch/vcooper/waveice_analysis/cesm23iws1tsks.ww3.hi.2014.nc')\n",
    "ww2015 = xr.open_dataset('/glade/scratch/vcooper/waveice_analysis/cesm23iws1tsks.ww3.hi.2015.nc')\n",
    "ww2016 = xr.open_dataset('/glade/scratch/vcooper/waveice_analysis/cesm23iws1tsks.ww3.hi.2016.nc')\n",
    "ww2017 = xr.open_dataset('/glade/scratch/vcooper/waveice_analysis/cesm23iws1tsks.ww3.hi.2017.nc')\n",
    "ww2018 = xr.open_dataset('/glade/scratch/vcooper/waveice_analysis/cesm23iws1tsks.ww3.hi.2018.nc')\n",
    "ww2019 = xr.open_dataset('/glade/scratch/vcooper/waveice_analysis/cesm23iws1tsks.ww3.hi.2019.nc')\n",
    "\n",
    "ww_dict = {'ww2012' : ww2012,\n",
    "           'ww2013' : ww2013,\n",
    "           'ww2014' : ww2014,\n",
    "           'ww2015' : ww2015,\n",
    "           'ww2016' : ww2016,\n",
    "           'ww2017' : ww2017,\n",
    "           'ww2018' : ww2018,\n",
    "           'ww2019' : ww2019}\n",
    "\n",
    "## get frequency domain from old data\n",
    "temp_f = xr.open_dataset('./ww1719ef_beau_cat.nc')\n",
    "\n",
    "for key,val in ww_dict.items():\n",
    "    print(key)\n",
    "    \n",
    "    ## rename vars to match the old model output\n",
    "    val = val.rename({'UAX': 'uwnd',\n",
    "                'UAY': 'vwnd',\n",
    "                'ICE': 'ice',\n",
    "                'HS':  'hs',\n",
    "                'T02': 't02',\n",
    "                'T0M1':'t0m1',\n",
    "                'T01': 't01',\n",
    "                'FP0': 'fp',\n",
    "                'THM': 'dir',\n",
    "                'EF':  'ef',\n",
    "                'FREQ':'f',\n",
    "                'NX':  'ni',\n",
    "                'NY':  'nj'})\n",
    "\n",
    "    val['latitude'] = (['nj','ni'],grid.lat.values)\n",
    "    val['longitude'] = (['nj','ni'],grid.lon.values)\n",
    "    val = val.set_coords(['time','latitude','longitude'])\n",
    "    val['f'] = temp_f.f\n",
    "    val = val.sel(nj=latslice)\n",
    "    val.coords['mask'] = (('nj','ni'), beau_mask)\n",
    "    \n",
    "    ## eliminate all data outside central beaufort region\n",
    "    val = val.where(val.mask > 0, drop=True)\n",
    "    ww_dict[key] = val\n",
    "\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ww_allbeau = xr.concat([ww_dict['ww2012'],\n",
    "                        ww_dict['ww2013'],\n",
    "                        ww_dict['ww2014'],\n",
    "                        ww_dict['ww2015'],\n",
    "                        ww_dict['ww2016'],\n",
    "                        ww_dict['ww2017'],\n",
    "                        ww_dict['ww2018'],\n",
    "                        ww_dict['ww2019']],dim='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## save netcdf of central beaufort 2012-2019 with all wavewatch and distance data\n",
    "new_filename = '/glade/scratch/vcooper/waveice_analysis/cesm23iws1tsks.ww3.hi.2012-2019.beau.nc'\n",
    "print ('saving to ', new_filename)\n",
    "\n",
    "ww_allbeau.to_netcdf(path=new_filename)\n",
    "print ('finished saving')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## then can load it and rename it to plug into existing code\n",
    "ww_allbeau = xr.open_dataset('/glade/scratch/vcooper/waveice_analysis/cesm23iws1tsks.ww3.hi.2012-2019.beau.nc')\n",
    "ww_allbeau = ww_allbeau.rename({'distance':'dist'})\n",
    "\n",
    "## note that when we load in the the 2012-2019 dataset from the new run,\n",
    "## we still keep the \"ww1719\" naming convention from when we only had \n",
    "## spectral output from 2017-2019. This is annoying because the '1719'\n",
    "## is a misnomer, but this way the analysis code didnt need to be changed\n",
    "## with the new run.\n",
    "ww1719ef_beau_cat = ww_allbeau\n",
    "ww1719beau_cat = ww_allbeau"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
